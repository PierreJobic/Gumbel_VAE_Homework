{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_clustering_empty.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_8ukRwk-QxbY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# VAE for MNIST clustering and generation\n",
        "\n",
        "The goal of this notebook is to explore some recent works dealing with variational auto-encoder (VAE).\n",
        "\n",
        "We will use MNIST dataset and a basic VAE architecture. "
      ]
    },
    {
      "metadata": {
        "id": "qXJGXRfLSlvH",
        "colab_type": "code",
        "outputId": "d212c5e0-4953-4ca3-c1d4-694427bd2436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x570b8000 @  0x7fb538bfb2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hp72MdbxV02_",
        "colab_type": "code",
        "outputId": "146f7f1f-77c6-4159-966d-985c9bfbac7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(accelerator)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "cu80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuI1UOySQxbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQgJp-uyQxbh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a directory if not exists\n",
        "sample_dir = 'samples'\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21wd7e0_Qxbl",
        "colab_type": "code",
        "outputId": "9fb784e5-6dc1-4c85-c6af-9bf0aa52de45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "#to be modified\n",
        "data_dir = '/home/mlelarge/data'\n",
        "# MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root=data_dir,\n",
        "                                     train=True,\n",
        "                                     transform=transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=10, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WJTwMcR7Qxbp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "Consider a latent variable model with a data variable $x\\in \\mathcal{X}$ and a latent variable $z\\in \\mathcal{Z}$, $p(z,x) = p(z)p_\\theta(x|z)$. Given the data $x_1,\\dots, x_n$, we want to train the model by maximizing the marginal log-likelihood:\n",
        "\\begin{eqnarray*}\n",
        "\\mathcal{L} = \\mathbf{E}_{p_d(x)}\\left[\\log p_\\theta(x)\\right]=\\mathbf{E}_{p_d(x)}\\left[\\log \\int_{\\mathcal{Z}}p_{\\theta}(x|z)p(z)dz\\right],\n",
        "  \\end{eqnarray*}\n",
        "  where $p_d$ denotes the empirical distribution of $X$: $p_d(x) =\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}(x)$.\n",
        "\n",
        " To avoid the (often) difficult computation of the integral above, the idea behind variational methods is to instea maximize a lower bound to the log-likelihood:\n",
        "  \\begin{eqnarray*}\n",
        "\\mathcal{L} \\geq L(p_\\theta(x|z),q(z|x)) =\\mathbf{E}_{p_d(x)}\\left[\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]-\\mathrm{KL}\\left( q(z|x)||p(z)\\right)\\right].\n",
        "  \\end{eqnarray*}\n",
        "  Any choice of $q(z|x)$ gives a valid lower bound. Variational autoencoders replace the variational posterior $q(z|x)$ by an inference network $q_{\\phi}(z|x)$ that is trained together with $p_{\\theta}(x|z)$ to jointly maximize $L(p_\\theta,q_\\phi)$. The variational posterior $q_{\\phi}(z|x)$ is also called the encoder and the generative model $p_{\\theta}(x|z)$, the decoder or generator.\n",
        "\n",
        "The first term $\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ is the negative reconstruction error. Indeed under a gaussian assumption i.e. $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), 1)$ the term $\\log p_\\theta(x|z)$ reduced to $\\propto \\|x-\\mu_\\theta(z)\\|^2$, which is often used in practice. The term $\\mathrm{KL}\\left( q(z|x)||p(z)\\right)$ can be seen as a regularization term, where the variational posterior $q_\\phi(z|x)$ should be matched to the prior $p(z)= \\mathcal{N}(0,1)$.\n",
        "\n",
        "Variational Autoencoders were introduced by [Kingma and Welling](https://arxiv.org/abs/1312.6114), see also [Doersch](https://arxiv.org/abs/1606.05908) for a tutorial.\n",
        "\n",
        "There are vairous examples of VAE in pytorch available [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65). The code below is taken from this last source."
      ]
    },
    {
      "metadata": {
        "id": "kEgqVJm2Qxbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "h_dim = 400\n",
        "z_dim = 20\n",
        "num_epochs = 15\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        \n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, image_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return torch.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ayCxgmMQQxbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here for the loss, instead of MSE for the reconstruction loss, we take BCE. The code below is still from the pytorch tutorial (with minor modifications to avoid warnings!)."
      ]
    },
    {
      "metadata": {
        "id": "BSewmKEdQxbv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, _) in enumerate(data_loader):\n",
        "        # Forward pass\n",
        "        x = x.to(device).view(-1, image_size)\n",
        "        x_reconst, mu, log_var = model(x)\n",
        "        \n",
        "        # Compute reconstruction loss and kl divergence\n",
        "        # For KL divergence, see Appendix B in VAE paper\n",
        "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
        "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKALOJ7_Qxbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let see how our network reconstructs our last batch. We display pairs of original digits and reconstructed version."
      ]
    },
    {
      "metadata": {
        "id": "H7_U6zg2Qxbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mu, _ = model.encode(x) \n",
        "out = model.decode(mu)\n",
        "x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mz6ENgFNQxb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let see now, how our network generates new samples."
      ]
    },
    {
      "metadata": {
        "id": "bP3bLwcPQxb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        z = torch.randn(16, z_dim).to(device)\n",
        "        out = model.decode(z).view(-1, 1, 28, 28)\n",
        "\n",
        "out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Spc82qLQxb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not great, but we did not train our network for long... That being said, we have no control of the generated digits. In the rest of this jupyter, we explore ways to generates zeros, ones, twos and so on. As a by product, we show how our VAE will allow us to do clustering.\n",
        "\n",
        "The main idea is to build what we call a Gumbel VAE as described below."
      ]
    },
    {
      "metadata": {
        "id": "41OFXM4tQxb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gumbel VAE\n",
        "\n",
        "Implement a VAE where you add a categorical variable $c\\in \\{0,\\dots 9\\}$ so that your latent variable model is $p(c,z,x) = p(c)p(z)p_{\\theta}(x|,c,z)$ and your variational posterior is $q_{\\phi}(c|x)q_{\\phi}(z|x)$ as described in this NIPS [paper](https://arxiv.org/abs/1804.00104). Make minimal modifications to previous architecture...\n",
        "\n",
        "The idea is that you incorporates a categorical variable in your latent space. You hope that this categorical variable will encode the class of the digit, so that your network can use it for a better reconstruction. Moreover, if things work as planed, you will then be able to generate digits conditionally to the class, i.e. you can choose the class thanks to the latent categorical variable $c$ and then generate digits from this class.\n",
        "\n",
        "As noticed above, in order to sample random variables while still being able to use backpropagation required us to use the reparameterization trick which is easy for Gaussian random variables. For categorical random variables, the reparameterization trick is explained in this [paper](https://arxiv.org/abs/1611.01144). This is implemented in pytorch thanks to [F.gumbel_softmax](https://pytorch.org/docs/stable/nn.html?highlight=gumbel_softmax#torch.nn.functional.gumbel_softmax)"
      ]
    },
    {
      "metadata": {
        "id": "Xk_Fcd-kQxb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "\n",
        "class VAE_Gumbel(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=20, n_classes = 10):\n",
        "        super(VAE_Gumbel, self).__init__()\n",
        "        #\n",
        "        # your code here\n",
        "        self.fc1 = nn.Linear(image_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        \n",
        "        ## I add a layer to encode the category class\n",
        "        self.fc4 = nn.Linear(h_dim, n_classes)\n",
        "\n",
        "        self.fc5 = nn.Linear(z_dim + n_classes, h_dim)\n",
        "        self.fc6 = nn.Linear(h_dim, image_size)\n",
        "        #\n",
        "        \n",
        "    def encode(self, x):\n",
        "        #\n",
        "        # your code here / use F.log_softmax\n",
        "        h = F.relu(self.fc1(x))\n",
        "        \n",
        "        h_1 = self.fc2(h)\n",
        "        h_2 = self.fc3(h)\n",
        "        c = self.fc4(h)\n",
        "        c = F.log_softmax(c, dim=1)\n",
        "        return h_1, h_2, c\n",
        "        #\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, y_onehot):\n",
        "        #\n",
        "        # your code here / use torch.cat\n",
        "        h = F.relu(self.fc5(torch.cat((z, y_onehot), dim=1)))\n",
        "        return torch.sigmoid(self.fc6(h))\n",
        "        #\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #\n",
        "        # your code here / use F.gumbel_softmax\n",
        "        mu, log_var, y_onehot = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z, F.gumbel_softmax(y_onehot))\n",
        "        return x_reconst, mu, log_var, y_onehot\n",
        "        #\n",
        "\n",
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dfPcKYFaQxb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You need to modify the loss to take into account the categorical random variable with an uniform prior on $\\{0,\\dots 9\\}$, see Appendix A.2 in the NIPS [paper](https://arxiv.org/abs/1804.00104)"
      ]
    },
    {
      "metadata": {
        "id": "p1KjI-tuQxcA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_G(model, data_loader=data_loader,num_epochs=num_epochs, beta = 1., verbose=True):\n",
        "    model.train(True)\n",
        "    for epoch in range(num_epochs):\n",
        "        all_labels = []\n",
        "        all_labels_est = []\n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            #\n",
        "            # your code here\n",
        "            x_reconst, mu, log_var, y_onehot = model(x)\n",
        "            \n",
        "            ## Transform labels into one_hot encoding to use as mask for kl_div of y_onehot||labels\n",
        "            labels_onehot = torch.FloatTensor(labels.size(0), 10)\n",
        "            labels_onehot.zero_()\n",
        "            labels_onehot.scatter_(1, labels.view(-1,1), 1)\n",
        "            # \n",
        "\n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            #\n",
        "            # your code here\n",
        "            kl_div_z = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            ## Like said in the Forum, to minimize KL_div_c we need to minize -log(p(c=i|x))\n",
        "            ## So i transformed labels into labels_onehot encoding so that the multiplication between y_onehot and labels_onehot\n",
        "            ## keeps only these -log(p(c=i|x)) and the computation is fast because it is a multiplication\n",
        "            kl_div_c = - torch.sum(y_onehot * labels_onehot.to(device))\n",
        "            #\n",
        "         \n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + beta * (kl_div_z + kl_div_c) # your code here\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 10 == 0:\n",
        "                    print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Entropy: {:.4f}\" \n",
        "                           .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size,\n",
        "                                   kl_div_z.item()/batch_size, kl_div_c.item()/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWw1ip4MQxcC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_G(model_G,num_epochs=10,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wr7gaaMxQxcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,_ = next(iter(data_loader))\n",
        "x = x[:24,:,:,:].to(device)\n",
        "out, _, _, log_p = model_G(x.view(-1, image_size)) \n",
        "x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJPS7IiAQxcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This was for reconstruction, but we care more about generation. For each category, we are generating 8 samples thanks to the following matrix, so that in the end, we should have on each line only one digit represented."
      ]
    },
    {
      "metadata": {
        "id": "RfJgP61RQxcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = np.zeros((8,n_classes))\n",
        "matrix[:,0] = 1\n",
        "final = matrix[:]\n",
        "for i in range(1,n_classes):\n",
        "    final = np.vstack((final,np.roll(matrix,i)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3FaETEvQxcJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "        y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "        out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "\n",
        "out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsdQDerIQxcL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It does not look like our original idea is working...\n",
        "\n",
        "To check that our network is not using the categorical variable, we can track the [normalized mutual information](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) between the true labels and the labels 'predicted' by our network (just by taking the category with maximal probability). Change your training loop to return the normalized mutual information (NMI) for each epoch. Plot the curve to check that the NMI is actually decreasing."
      ]
    },
    {
      "metadata": {
        "id": "GE-E_I5saroc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUOPHuoEN6OT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_G_NMI(model, data_loader=data_loader,num_epochs=num_epochs, beta = 1.0, verbose=True):\n",
        "    model.train(True)\n",
        "    NMI = []\n",
        "    for epoch in range(num_epochs):\n",
        "        all_labels = []\n",
        "        all_labels_est = []\n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            x_reconst, mu, log_var, y_onehot = model(x)\n",
        "            \n",
        "            # Transform labels into one_hot encoding to compare with latent distribution y_onehot\n",
        "            labels_onehot = torch.FloatTensor(labels.size(0), 10)\n",
        "            labels_onehot.zero_()\n",
        "            labels_onehot.scatter_(1, labels.view(-1,1), 1)\n",
        "            \n",
        "            \n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            kl_div_z = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            kl_div_c = - torch.sum(y_onehot * labels_onehot.to(device))\n",
        "            \n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + beta * (kl_div_z + kl_div_c) # your code here\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 10 == 0:\n",
        "                    NMI_print = normalized_mutual_info_score(labels.detach().cpu().numpy(), torch.max(y_onehot, dim=1)[1].detach().cpu().numpy())\n",
        "                    NMI.append(NMI_print)\n",
        "                    print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Entropy: {:.4f}, NMI: {:.4f}\" \n",
        "                           .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size,\n",
        "                                   kl_div_z.item()/batch_size, kl_div_c.item()/batch_size, NMI_print))\n",
        "    return NMI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCur4PeWOeNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NMI = train_G_NMI(model_G,num_epochs=10,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOW3uhHkcj4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(NMI)\n",
        "\n",
        "## NMI is increasing but I think it is normal. Because as we can see on the generation image ( the last figure ), \n",
        "## we see that 1 is almost recognized ( the second row ) and the other rows are not completely random ( last row looks like 9 ).\n",
        "## It means that our network learnt some categorical information but it is not enough to have a good generation. Lets see what happen in next section"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UR_kmvQAQxcM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to force our network to use the categorical variable, we will change the loss following this ICLR [paper](https://openreview.net/forum?id=Sy2fzU9gl)\n",
        "\n",
        "Implement this change in the training loop and plot the new NMI curve after 10 epochs. For $\\beta = 20$, you should see that NMI increases. But reconstruction starts to be bad and generation is still poor.\n",
        "\n",
        "This is explained in this [paper](https://arxiv.org/abs/1804.03599) and a solution is proposed see Section 5. Implement the solution described in Section 3 equation (7) if the NIPS [paper](https://arxiv.org/abs/1804.00104) "
      ]
    },
    {
      "metadata": {
        "id": "S22JHPXLg93T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)\n",
        "def train_G_NMI_beta20(model, data_loader=data_loader,num_epochs=num_epochs, beta = 20., verbose=True):\n",
        "    model.train(True)\n",
        "    NMI = []\n",
        "    for epoch in range(num_epochs):\n",
        "        all_labels = []\n",
        "        all_labels_est = []\n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            x_reconst, mu, log_var, y_onehot = model(x)\n",
        "            \n",
        "            # Transform labels into one_hot encoding to compare with latent distribution y_onehot\n",
        "            labels_onehot = torch.FloatTensor(labels.size(0), 10)\n",
        "            labels_onehot.zero_()\n",
        "            labels_onehot.scatter_(1, labels.view(-1,1), 1)\n",
        "            \n",
        "            \n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            kl_div_z = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            kl_div_c = - torch.sum(y_onehot * labels_onehot.to(device))\n",
        "            \n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + beta * (kl_div_z + kl_div_c) # your code here\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 10 == 0:\n",
        "                    NMI_print = normalized_mutual_info_score(labels.detach().cpu().numpy(), torch.max(y_onehot, dim=1)[1].detach().cpu().numpy())\n",
        "                    NMI.append(NMI_print)\n",
        "                   \n",
        "    plt.plot(NMI)\n",
        "    plt.show()\n",
        "    \n",
        "train_G_NMI_beta20(model_G,num_epochs=10,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfOx5MNZftQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here the NMI is closer to 1 compared to the last one. Hence our network learned more categorical information"
      ]
    },
    {
      "metadata": {
        "id": "0LHfwabwikTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,_ = next(iter(data_loader))\n",
        "x = x[:24,:,:,:].to(device)\n",
        "out, _, _, log_p = model_G(x.view(-1, image_size)) \n",
        "x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUeeknUhf5yw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The reconstruction is bad, digits look more like ghost digits"
      ]
    },
    {
      "metadata": {
        "id": "eIrfzWGZioVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "        y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "        out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "\n",
        "out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AA3LPNnwgMZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generation is quite good but digits still look like ghost digits which is not what we want."
      ]
    },
    {
      "metadata": {
        "id": "M_AjvD9ci1G7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution proposed in Section 5 of NIPS paper"
      ]
    },
    {
      "metadata": {
        "id": "qcqu3nV4QxcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9W6Iw7i1QxcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# proposed value for MNIST in the paper:\n",
        "# beta = 30\n",
        "# C_z = 0 to 5 in 25000 iterations\n",
        "# C_c = 0 to 5 in 25000 iterations\n",
        "\n",
        "def train_G_modified_loss(model, data_loader=data_loader,num_epochs=num_epochs, beta=30. , C_z_fin=5., C_c_fin=5., verbose=True):\n",
        "    #\n",
        "    # your code here\n",
        "    model.train(True)\n",
        "    NMI = []\n",
        "    C_z = 0\n",
        "    C_c = 0\n",
        "    C = num_epochs * len(data_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        all_labels = []\n",
        "        all_labels_est = []\n",
        "        for i, (x, labels) in enumerate(data_loader):\n",
        "          \n",
        "            # Forward pass\n",
        "            x = x.to(device).view(-1, image_size)\n",
        "            x_reconst, mu, log_var, y_onehot = model(x)\n",
        "            \n",
        "            # Transform labels into one_hot encoding to compare with latent distribution y_onehot\n",
        "            labels_onehot = torch.FloatTensor(labels.size(0), 10)\n",
        "            labels_onehot.zero_()\n",
        "            labels_onehot.scatter_(1, labels.view(-1,1), 1)\n",
        "            \n",
        "            \n",
        "            reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
        "            kl_div_z = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            kl_div_c = - torch.sum(y_onehot * labels_onehot.to(device))\n",
        "            \n",
        "            # Backprop and optimize\n",
        "            loss = reconst_loss + beta * (abs(kl_div_z - C_z)  + abs(kl_div_c - C_c)) # your code here\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            C_z += C_z_fin/C\n",
        "            C_c += C_c_fin/C\n",
        "            \n",
        "            if verbose:\n",
        "                if (i+1) % 10 == 0:\n",
        "                    NMI_print = normalized_mutual_info_score(labels.detach().cpu().numpy(), torch.max(y_onehot, dim=1)[1].detach().cpu().numpy())\n",
        "                    NMI.append(NMI_print)\n",
        "                    print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Entropy: {:.4f}, NMI: {:.4f}\" \n",
        "                           .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size,\n",
        "                                   kl_div_z.item()/batch_size, kl_div_c.item()/batch_size, NMI_print))\n",
        "                    \n",
        "                   \n",
        "    return NMI\n",
        "    #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1_6OKuzlS7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NMI = train_G_modified_loss(model_G,num_epochs=10,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jm5f7H1Vi0Ku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(NMI)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3PbYbOYjQxcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "        y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "        out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbRKa9bHQxca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "with torch.no_grad():\n",
        "    plt.plot()\n",
        "    z = torch.randn(8, z_dim).to(device)\n",
        "    y_onehot = torch.tensor(np.roll(matrix,i)).type(torch.FloatTensor).to(device)\n",
        "    out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "    show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjgFFkhwQxcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,_ = next(iter(data_loader))\n",
        "x = x[:24,:,:,:].to(device)\n",
        "out, _, _, log_p = model_G(x.view(-1, image_size)) \n",
        "x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "show(out_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lew61r-AmuW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1jF1O1A68W1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Custom Solution ( different beta, C_z, C_c ) while keeping epochs to 10 to be comparable"
      ]
    },
    {
      "metadata": {
        "id": "5h4mqEk37D8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7745
        },
        "outputId": "6d4e6e18-1746-422a-a946-784c06dc0dce"
      },
      "cell_type": "code",
      "source": [
        "model_G = VAE_Gumbel().to(device)\n",
        "optimizer = torch.optim.Adam(model_G.parameters(), lr=learning_rate)\n",
        "NMI = train_G_modified_loss(model_G,num_epochs=10, beta = 4, C_z_fin=5., C_c_fin=10., verbose=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[1/10], Step [10/469], Reconst Loss: 319.0919, KL Div: 7.9159, Entropy: 2.0909, NMI: 0.4566\n",
            "Epoch[1/10], Step [20/469], Reconst Loss: 246.1782, KL Div: 3.6240, Entropy: 1.9570, NMI: 0.5667\n",
            "Epoch[1/10], Step [30/469], Reconst Loss: 222.5212, KL Div: 1.8979, Entropy: 1.8466, NMI: 0.6717\n",
            "Epoch[1/10], Step [40/469], Reconst Loss: 214.5994, KL Div: 1.5086, Entropy: 1.6266, NMI: 0.7246\n",
            "Epoch[1/10], Step [50/469], Reconst Loss: 209.2282, KL Div: 1.8736, Entropy: 1.2427, NMI: 0.7572\n",
            "Epoch[1/10], Step [60/469], Reconst Loss: 208.3428, KL Div: 1.8602, Entropy: 1.0526, NMI: 0.6532\n",
            "Epoch[1/10], Step [70/469], Reconst Loss: 204.4044, KL Div: 1.7307, Entropy: 0.9514, NMI: 0.6853\n",
            "Epoch[1/10], Step [80/469], Reconst Loss: 198.4633, KL Div: 2.4303, Entropy: 0.6005, NMI: 0.8396\n",
            "Epoch[1/10], Step [90/469], Reconst Loss: 190.4949, KL Div: 2.9288, Entropy: 0.6597, NMI: 0.7517\n",
            "Epoch[1/10], Step [100/469], Reconst Loss: 192.3248, KL Div: 2.7293, Entropy: 0.5043, NMI: 0.7974\n",
            "Epoch[1/10], Step [110/469], Reconst Loss: 186.6008, KL Div: 2.9163, Entropy: 0.5802, NMI: 0.8064\n",
            "Epoch[1/10], Step [120/469], Reconst Loss: 176.0044, KL Div: 3.2579, Entropy: 0.4304, NMI: 0.8590\n",
            "Epoch[1/10], Step [130/469], Reconst Loss: 176.1010, KL Div: 3.9192, Entropy: 0.4494, NMI: 0.8433\n",
            "Epoch[1/10], Step [140/469], Reconst Loss: 172.7367, KL Div: 3.6841, Entropy: 0.4032, NMI: 0.8407\n",
            "Epoch[1/10], Step [150/469], Reconst Loss: 165.9821, KL Div: 4.0343, Entropy: 0.4508, NMI: 0.8218\n",
            "Epoch[1/10], Step [160/469], Reconst Loss: 165.0764, KL Div: 4.3081, Entropy: 0.4091, NMI: 0.8216\n",
            "Epoch[1/10], Step [170/469], Reconst Loss: 163.0311, KL Div: 4.3766, Entropy: 0.4138, NMI: 0.8113\n",
            "Epoch[1/10], Step [180/469], Reconst Loss: 165.2760, KL Div: 4.4624, Entropy: 0.3522, NMI: 0.8173\n",
            "Epoch[1/10], Step [190/469], Reconst Loss: 157.6234, KL Div: 4.4326, Entropy: 0.2438, NMI: 0.9020\n",
            "Epoch[1/10], Step [200/469], Reconst Loss: 155.5146, KL Div: 4.3715, Entropy: 0.5431, NMI: 0.7699\n",
            "Epoch[1/10], Step [210/469], Reconst Loss: 158.2111, KL Div: 4.5256, Entropy: 0.3269, NMI: 0.8477\n",
            "Epoch[1/10], Step [220/469], Reconst Loss: 160.3487, KL Div: 4.6442, Entropy: 0.4902, NMI: 0.8431\n",
            "Epoch[1/10], Step [230/469], Reconst Loss: 154.6749, KL Div: 4.5570, Entropy: 0.3464, NMI: 0.8576\n",
            "Epoch[1/10], Step [240/469], Reconst Loss: 157.1408, KL Div: 4.8189, Entropy: 0.2885, NMI: 0.8494\n",
            "Epoch[1/10], Step [250/469], Reconst Loss: 155.0652, KL Div: 4.6474, Entropy: 0.4195, NMI: 0.8170\n",
            "Epoch[1/10], Step [260/469], Reconst Loss: 151.4466, KL Div: 4.6544, Entropy: 0.2774, NMI: 0.9152\n",
            "Epoch[1/10], Step [270/469], Reconst Loss: 151.6846, KL Div: 4.6411, Entropy: 0.2542, NMI: 0.9048\n",
            "Epoch[1/10], Step [280/469], Reconst Loss: 148.9489, KL Div: 4.8799, Entropy: 0.1856, NMI: 0.9512\n",
            "Epoch[1/10], Step [290/469], Reconst Loss: 146.5157, KL Div: 4.8128, Entropy: 0.2594, NMI: 0.8245\n",
            "Epoch[1/10], Step [300/469], Reconst Loss: 143.4244, KL Div: 4.7945, Entropy: 0.2832, NMI: 0.8926\n",
            "Epoch[1/10], Step [310/469], Reconst Loss: 148.1497, KL Div: 5.0311, Entropy: 0.3092, NMI: 0.8822\n",
            "Epoch[1/10], Step [320/469], Reconst Loss: 155.0154, KL Div: 5.1187, Entropy: 0.2354, NMI: 0.9009\n",
            "Epoch[1/10], Step [330/469], Reconst Loss: 142.3756, KL Div: 4.6603, Entropy: 0.3036, NMI: 0.8902\n",
            "Epoch[1/10], Step [340/469], Reconst Loss: 145.4803, KL Div: 4.8849, Entropy: 0.1391, NMI: 0.9084\n",
            "Epoch[1/10], Step [350/469], Reconst Loss: 149.6725, KL Div: 4.8248, Entropy: 0.2584, NMI: 0.8991\n",
            "Epoch[1/10], Step [360/469], Reconst Loss: 144.5560, KL Div: 4.7243, Entropy: 0.2710, NMI: 0.8659\n",
            "Epoch[1/10], Step [370/469], Reconst Loss: 140.9665, KL Div: 4.7681, Entropy: 0.2009, NMI: 0.9267\n",
            "Epoch[1/10], Step [380/469], Reconst Loss: 152.0221, KL Div: 4.8628, Entropy: 0.2894, NMI: 0.8822\n",
            "Epoch[1/10], Step [390/469], Reconst Loss: 149.2764, KL Div: 4.7948, Entropy: 0.1829, NMI: 0.9092\n",
            "Epoch[1/10], Step [400/469], Reconst Loss: 141.4567, KL Div: 4.8629, Entropy: 0.3726, NMI: 0.8610\n",
            "Epoch[1/10], Step [410/469], Reconst Loss: 134.7731, KL Div: 4.5581, Entropy: 0.2542, NMI: 0.8836\n",
            "Epoch[1/10], Step [420/469], Reconst Loss: 147.3178, KL Div: 5.2511, Entropy: 0.3658, NMI: 0.8619\n",
            "Epoch[1/10], Step [430/469], Reconst Loss: 146.8432, KL Div: 5.0698, Entropy: 0.2057, NMI: 0.8944\n",
            "Epoch[1/10], Step [440/469], Reconst Loss: 136.5230, KL Div: 4.9236, Entropy: 0.1775, NMI: 0.9294\n",
            "Epoch[1/10], Step [450/469], Reconst Loss: 142.8657, KL Div: 4.8603, Entropy: 0.3872, NMI: 0.8083\n",
            "Epoch[1/10], Step [460/469], Reconst Loss: 142.8616, KL Div: 5.0130, Entropy: 0.2665, NMI: 0.9033\n",
            "Epoch[2/10], Step [10/469], Reconst Loss: 137.3458, KL Div: 4.9678, Entropy: 0.2942, NMI: 0.8553\n",
            "Epoch[2/10], Step [20/469], Reconst Loss: 143.7926, KL Div: 4.6207, Entropy: 0.2700, NMI: 0.8896\n",
            "Epoch[2/10], Step [30/469], Reconst Loss: 137.1950, KL Div: 5.0475, Entropy: 0.1997, NMI: 0.8827\n",
            "Epoch[2/10], Step [40/469], Reconst Loss: 139.0073, KL Div: 4.6086, Entropy: 0.2822, NMI: 0.8813\n",
            "Epoch[2/10], Step [50/469], Reconst Loss: 135.4340, KL Div: 4.8326, Entropy: 0.3713, NMI: 0.8587\n",
            "Epoch[2/10], Step [60/469], Reconst Loss: 140.2110, KL Div: 5.0319, Entropy: 0.4407, NMI: 0.7798\n",
            "Epoch[2/10], Step [70/469], Reconst Loss: 140.1745, KL Div: 4.8458, Entropy: 0.1302, NMI: 0.9501\n",
            "Epoch[2/10], Step [80/469], Reconst Loss: 140.8285, KL Div: 4.9807, Entropy: 0.2139, NMI: 0.8606\n",
            "Epoch[2/10], Step [90/469], Reconst Loss: 137.0944, KL Div: 5.1221, Entropy: 0.4225, NMI: 0.8183\n",
            "Epoch[2/10], Step [100/469], Reconst Loss: 138.3377, KL Div: 4.9519, Entropy: 0.3750, NMI: 0.8350\n",
            "Epoch[2/10], Step [110/469], Reconst Loss: 137.9031, KL Div: 4.8228, Entropy: 0.2285, NMI: 0.9094\n",
            "Epoch[2/10], Step [120/469], Reconst Loss: 132.9872, KL Div: 4.8835, Entropy: 0.2390, NMI: 0.9021\n",
            "Epoch[2/10], Step [130/469], Reconst Loss: 131.1227, KL Div: 5.0278, Entropy: 0.2013, NMI: 0.9373\n",
            "Epoch[2/10], Step [140/469], Reconst Loss: 133.8820, KL Div: 4.6249, Entropy: 0.1996, NMI: 0.8926\n",
            "Epoch[2/10], Step [150/469], Reconst Loss: 138.1415, KL Div: 5.1523, Entropy: 0.3011, NMI: 0.8607\n",
            "Epoch[2/10], Step [160/469], Reconst Loss: 136.2612, KL Div: 4.6282, Entropy: 0.2553, NMI: 0.8615\n",
            "Epoch[2/10], Step [170/469], Reconst Loss: 136.4340, KL Div: 4.9659, Entropy: 0.3411, NMI: 0.8178\n",
            "Epoch[2/10], Step [180/469], Reconst Loss: 123.0426, KL Div: 4.9493, Entropy: 0.3069, NMI: 0.8801\n",
            "Epoch[2/10], Step [190/469], Reconst Loss: 133.4513, KL Div: 4.8998, Entropy: 0.2878, NMI: 0.8910\n",
            "Epoch[2/10], Step [200/469], Reconst Loss: 141.4128, KL Div: 5.2926, Entropy: 0.1937, NMI: 0.8782\n",
            "Epoch[2/10], Step [210/469], Reconst Loss: 136.9100, KL Div: 5.1320, Entropy: 0.3037, NMI: 0.8421\n",
            "Epoch[2/10], Step [220/469], Reconst Loss: 133.7914, KL Div: 4.7075, Entropy: 0.3316, NMI: 0.8263\n",
            "Epoch[2/10], Step [230/469], Reconst Loss: 135.5867, KL Div: 4.9350, Entropy: 0.3059, NMI: 0.8606\n",
            "Epoch[2/10], Step [240/469], Reconst Loss: 134.2979, KL Div: 5.0744, Entropy: 0.2563, NMI: 0.8592\n",
            "Epoch[2/10], Step [250/469], Reconst Loss: 131.6112, KL Div: 4.9973, Entropy: 0.2540, NMI: 0.8695\n",
            "Epoch[2/10], Step [260/469], Reconst Loss: 127.3649, KL Div: 4.8393, Entropy: 0.2520, NMI: 0.8641\n",
            "Epoch[2/10], Step [270/469], Reconst Loss: 129.9344, KL Div: 4.9704, Entropy: 0.1776, NMI: 0.9260\n",
            "Epoch[2/10], Step [280/469], Reconst Loss: 132.8812, KL Div: 4.7568, Entropy: 0.3681, NMI: 0.8710\n",
            "Epoch[2/10], Step [290/469], Reconst Loss: 130.1202, KL Div: 5.1160, Entropy: 0.3080, NMI: 0.8545\n",
            "Epoch[2/10], Step [300/469], Reconst Loss: 136.0782, KL Div: 5.1654, Entropy: 0.2481, NMI: 0.9134\n",
            "Epoch[2/10], Step [310/469], Reconst Loss: 128.7265, KL Div: 4.9132, Entropy: 0.2200, NMI: 0.9159\n",
            "Epoch[2/10], Step [320/469], Reconst Loss: 125.9881, KL Div: 4.9092, Entropy: 0.2322, NMI: 0.9032\n",
            "Epoch[2/10], Step [330/469], Reconst Loss: 132.4211, KL Div: 4.8706, Entropy: 0.2903, NMI: 0.8701\n",
            "Epoch[2/10], Step [340/469], Reconst Loss: 127.5184, KL Div: 4.9681, Entropy: 0.2909, NMI: 0.8118\n",
            "Epoch[2/10], Step [350/469], Reconst Loss: 134.9979, KL Div: 5.0597, Entropy: 0.2074, NMI: 0.8950\n",
            "Epoch[2/10], Step [360/469], Reconst Loss: 126.2835, KL Div: 4.9784, Entropy: 0.2527, NMI: 0.8418\n",
            "Epoch[2/10], Step [370/469], Reconst Loss: 131.1509, KL Div: 5.1171, Entropy: 0.2456, NMI: 0.8687\n",
            "Epoch[2/10], Step [380/469], Reconst Loss: 137.4474, KL Div: 5.3246, Entropy: 0.3111, NMI: 0.8810\n",
            "Epoch[2/10], Step [390/469], Reconst Loss: 138.7915, KL Div: 5.6886, Entropy: 0.3325, NMI: 0.8330\n",
            "Epoch[2/10], Step [400/469], Reconst Loss: 131.8118, KL Div: 4.9220, Entropy: 0.2566, NMI: 0.9038\n",
            "Epoch[2/10], Step [410/469], Reconst Loss: 128.6205, KL Div: 5.1947, Entropy: 0.4051, NMI: 0.8535\n",
            "Epoch[2/10], Step [420/469], Reconst Loss: 130.0791, KL Div: 4.9152, Entropy: 0.3735, NMI: 0.8239\n",
            "Epoch[2/10], Step [430/469], Reconst Loss: 128.6566, KL Div: 5.0936, Entropy: 0.2741, NMI: 0.8870\n",
            "Epoch[2/10], Step [440/469], Reconst Loss: 126.5822, KL Div: 5.1585, Entropy: 0.2372, NMI: 0.8893\n",
            "Epoch[2/10], Step [450/469], Reconst Loss: 130.7097, KL Div: 5.1935, Entropy: 0.1924, NMI: 0.9022\n",
            "Epoch[2/10], Step [460/469], Reconst Loss: 130.4682, KL Div: 5.5366, Entropy: 0.2773, NMI: 0.9052\n",
            "Epoch[3/10], Step [10/469], Reconst Loss: 129.6766, KL Div: 5.1736, Entropy: 0.1484, NMI: 0.9202\n",
            "Epoch[3/10], Step [20/469], Reconst Loss: 127.0949, KL Div: 4.9806, Entropy: 0.2933, NMI: 0.8744\n",
            "Epoch[3/10], Step [30/469], Reconst Loss: 129.1386, KL Div: 4.9267, Entropy: 0.3340, NMI: 0.8384\n",
            "Epoch[3/10], Step [40/469], Reconst Loss: 124.5255, KL Div: 5.0936, Entropy: 0.3023, NMI: 0.9129\n",
            "Epoch[3/10], Step [50/469], Reconst Loss: 130.4253, KL Div: 4.8424, Entropy: 0.1325, NMI: 0.9252\n",
            "Epoch[3/10], Step [60/469], Reconst Loss: 125.1823, KL Div: 5.0810, Entropy: 0.1693, NMI: 0.9215\n",
            "Epoch[3/10], Step [70/469], Reconst Loss: 126.1937, KL Div: 4.9224, Entropy: 0.3524, NMI: 0.8707\n",
            "Epoch[3/10], Step [80/469], Reconst Loss: 128.0076, KL Div: 4.9911, Entropy: 0.1756, NMI: 0.9148\n",
            "Epoch[3/10], Step [90/469], Reconst Loss: 123.3393, KL Div: 5.0955, Entropy: 0.1418, NMI: 0.9165\n",
            "Epoch[3/10], Step [100/469], Reconst Loss: 126.7868, KL Div: 5.1795, Entropy: 0.2881, NMI: 0.8253\n",
            "Epoch[3/10], Step [110/469], Reconst Loss: 127.0507, KL Div: 5.0006, Entropy: 0.1910, NMI: 0.9190\n",
            "Epoch[3/10], Step [120/469], Reconst Loss: 132.5831, KL Div: 5.2334, Entropy: 0.4421, NMI: 0.8615\n",
            "Epoch[3/10], Step [130/469], Reconst Loss: 122.3327, KL Div: 5.0078, Entropy: 0.2410, NMI: 0.9316\n",
            "Epoch[3/10], Step [140/469], Reconst Loss: 124.3646, KL Div: 4.9690, Entropy: 0.2018, NMI: 0.8586\n",
            "Epoch[3/10], Step [150/469], Reconst Loss: 124.4322, KL Div: 5.0707, Entropy: 0.1856, NMI: 0.9021\n",
            "Epoch[3/10], Step [160/469], Reconst Loss: 128.0142, KL Div: 5.1358, Entropy: 0.3214, NMI: 0.8606\n",
            "Epoch[3/10], Step [170/469], Reconst Loss: 125.1827, KL Div: 5.0609, Entropy: 0.2557, NMI: 0.9159\n",
            "Epoch[3/10], Step [180/469], Reconst Loss: 128.0061, KL Div: 5.5621, Entropy: 0.2526, NMI: 0.8485\n",
            "Epoch[3/10], Step [190/469], Reconst Loss: 123.9184, KL Div: 5.0858, Entropy: 0.2387, NMI: 0.9080\n",
            "Epoch[3/10], Step [200/469], Reconst Loss: 122.8027, KL Div: 5.0377, Entropy: 0.1559, NMI: 0.9407\n",
            "Epoch[3/10], Step [210/469], Reconst Loss: 126.4130, KL Div: 5.3147, Entropy: 0.2652, NMI: 0.8701\n",
            "Epoch[3/10], Step [220/469], Reconst Loss: 125.2958, KL Div: 5.0244, Entropy: 0.1241, NMI: 0.9527\n",
            "Epoch[3/10], Step [230/469], Reconst Loss: 129.6371, KL Div: 5.2744, Entropy: 0.3193, NMI: 0.8340\n",
            "Epoch[3/10], Step [240/469], Reconst Loss: 126.3683, KL Div: 5.1193, Entropy: 0.1329, NMI: 0.9043\n",
            "Epoch[3/10], Step [250/469], Reconst Loss: 127.3475, KL Div: 5.4816, Entropy: 0.4033, NMI: 0.8539\n",
            "Epoch[3/10], Step [260/469], Reconst Loss: 126.8092, KL Div: 5.0660, Entropy: 0.2173, NMI: 0.8507\n",
            "Epoch[3/10], Step [270/469], Reconst Loss: 126.8331, KL Div: 5.2126, Entropy: 0.1491, NMI: 0.9436\n",
            "Epoch[3/10], Step [280/469], Reconst Loss: 120.2625, KL Div: 5.0008, Entropy: 0.2303, NMI: 0.9157\n",
            "Epoch[3/10], Step [290/469], Reconst Loss: 124.4728, KL Div: 5.0457, Entropy: 0.3972, NMI: 0.8261\n",
            "Epoch[3/10], Step [300/469], Reconst Loss: 128.7603, KL Div: 5.2413, Entropy: 0.2494, NMI: 0.9062\n",
            "Epoch[3/10], Step [310/469], Reconst Loss: 120.3884, KL Div: 5.4669, Entropy: 0.2141, NMI: 0.8838\n",
            "Epoch[3/10], Step [320/469], Reconst Loss: 126.3948, KL Div: 5.3852, Entropy: 0.2718, NMI: 0.8301\n",
            "Epoch[3/10], Step [330/469], Reconst Loss: 128.4009, KL Div: 5.3981, Entropy: 0.3515, NMI: 0.8385\n",
            "Epoch[3/10], Step [340/469], Reconst Loss: 124.5251, KL Div: 5.2707, Entropy: 0.2990, NMI: 0.8596\n",
            "Epoch[3/10], Step [350/469], Reconst Loss: 128.2385, KL Div: 5.4560, Entropy: 0.2376, NMI: 0.8931\n",
            "Epoch[3/10], Step [360/469], Reconst Loss: 125.1670, KL Div: 5.2527, Entropy: 0.3943, NMI: 0.8539\n",
            "Epoch[3/10], Step [370/469], Reconst Loss: 127.5851, KL Div: 5.6847, Entropy: 0.2380, NMI: 0.8883\n",
            "Epoch[3/10], Step [380/469], Reconst Loss: 124.1130, KL Div: 5.0020, Entropy: 0.1804, NMI: 0.9055\n",
            "Epoch[3/10], Step [390/469], Reconst Loss: 125.2536, KL Div: 5.0670, Entropy: 0.2730, NMI: 0.9086\n",
            "Epoch[3/10], Step [400/469], Reconst Loss: 122.2386, KL Div: 5.0304, Entropy: 0.2467, NMI: 0.8748\n",
            "Epoch[3/10], Step [410/469], Reconst Loss: 123.9472, KL Div: 5.2510, Entropy: 0.2842, NMI: 0.8880\n",
            "Epoch[3/10], Step [420/469], Reconst Loss: 124.9224, KL Div: 5.3128, Entropy: 0.1799, NMI: 0.9414\n",
            "Epoch[3/10], Step [430/469], Reconst Loss: 126.4247, KL Div: 5.2729, Entropy: 0.2460, NMI: 0.8784\n",
            "Epoch[3/10], Step [440/469], Reconst Loss: 123.1517, KL Div: 5.5054, Entropy: 0.2271, NMI: 0.8829\n",
            "Epoch[3/10], Step [450/469], Reconst Loss: 128.7558, KL Div: 5.4474, Entropy: 0.1828, NMI: 0.9383\n",
            "Epoch[3/10], Step [460/469], Reconst Loss: 119.7566, KL Div: 5.2036, Entropy: 0.3168, NMI: 0.8289\n",
            "Epoch[4/10], Step [10/469], Reconst Loss: 133.3434, KL Div: 5.4514, Entropy: 0.3102, NMI: 0.8655\n",
            "Epoch[4/10], Step [20/469], Reconst Loss: 126.5243, KL Div: 5.2521, Entropy: 0.1411, NMI: 0.8890\n",
            "Epoch[4/10], Step [30/469], Reconst Loss: 122.2912, KL Div: 5.3474, Entropy: 0.1670, NMI: 0.9427\n",
            "Epoch[4/10], Step [40/469], Reconst Loss: 122.1836, KL Div: 5.3078, Entropy: 0.2849, NMI: 0.8908\n",
            "Epoch[4/10], Step [50/469], Reconst Loss: 125.6238, KL Div: 5.5506, Entropy: 0.1752, NMI: 0.8945\n",
            "Epoch[4/10], Step [60/469], Reconst Loss: 129.2398, KL Div: 5.1962, Entropy: 0.2220, NMI: 0.8471\n",
            "Epoch[4/10], Step [70/469], Reconst Loss: 127.9302, KL Div: 5.2714, Entropy: 0.1750, NMI: 0.9030\n",
            "Epoch[4/10], Step [80/469], Reconst Loss: 121.3546, KL Div: 5.3998, Entropy: 0.2035, NMI: 0.9121\n",
            "Epoch[4/10], Step [90/469], Reconst Loss: 124.9059, KL Div: 5.3010, Entropy: 0.2018, NMI: 0.8922\n",
            "Epoch[4/10], Step [100/469], Reconst Loss: 125.7090, KL Div: 5.2026, Entropy: 0.1927, NMI: 0.8697\n",
            "Epoch[4/10], Step [110/469], Reconst Loss: 118.3760, KL Div: 5.3183, Entropy: 0.1527, NMI: 0.9306\n",
            "Epoch[4/10], Step [120/469], Reconst Loss: 121.5317, KL Div: 5.3563, Entropy: 0.1572, NMI: 0.9175\n",
            "Epoch[4/10], Step [130/469], Reconst Loss: 124.8054, KL Div: 5.5115, Entropy: 0.2814, NMI: 0.9031\n",
            "Epoch[4/10], Step [140/469], Reconst Loss: 121.8272, KL Div: 4.9649, Entropy: 0.2087, NMI: 0.8948\n",
            "Epoch[4/10], Step [150/469], Reconst Loss: 119.5206, KL Div: 5.5591, Entropy: 0.1846, NMI: 0.9144\n",
            "Epoch[4/10], Step [160/469], Reconst Loss: 124.4890, KL Div: 5.1869, Entropy: 0.3867, NMI: 0.8517\n",
            "Epoch[4/10], Step [170/469], Reconst Loss: 116.3863, KL Div: 5.3418, Entropy: 0.1957, NMI: 0.9103\n",
            "Epoch[4/10], Step [180/469], Reconst Loss: 123.8384, KL Div: 5.2711, Entropy: 0.1011, NMI: 0.9515\n",
            "Epoch[4/10], Step [190/469], Reconst Loss: 120.5925, KL Div: 5.4409, Entropy: 0.0598, NMI: 1.0000\n",
            "Epoch[4/10], Step [200/469], Reconst Loss: 129.1379, KL Div: 5.3587, Entropy: 0.1941, NMI: 0.8930\n",
            "Epoch[4/10], Step [210/469], Reconst Loss: 117.2036, KL Div: 5.2409, Entropy: 0.2000, NMI: 0.8888\n",
            "Epoch[4/10], Step [220/469], Reconst Loss: 119.3745, KL Div: 5.2256, Entropy: 0.2341, NMI: 0.9089\n",
            "Epoch[4/10], Step [230/469], Reconst Loss: 122.5221, KL Div: 5.5341, Entropy: 0.2195, NMI: 0.8827\n",
            "Epoch[4/10], Step [240/469], Reconst Loss: 123.0678, KL Div: 5.1966, Entropy: 0.2251, NMI: 0.9197\n",
            "Epoch[4/10], Step [250/469], Reconst Loss: 115.2877, KL Div: 5.3809, Entropy: 0.1626, NMI: 0.9027\n",
            "Epoch[4/10], Step [260/469], Reconst Loss: 116.0956, KL Div: 5.1327, Entropy: 0.2729, NMI: 0.8802\n",
            "Epoch[4/10], Step [270/469], Reconst Loss: 120.1213, KL Div: 4.9458, Entropy: 0.2365, NMI: 0.8817\n",
            "Epoch[4/10], Step [280/469], Reconst Loss: 120.2896, KL Div: 5.3846, Entropy: 0.1585, NMI: 0.9199\n",
            "Epoch[4/10], Step [290/469], Reconst Loss: 116.2662, KL Div: 5.0288, Entropy: 0.2531, NMI: 0.8531\n",
            "Epoch[4/10], Step [300/469], Reconst Loss: 123.3022, KL Div: 5.6741, Entropy: 0.2887, NMI: 0.8724\n",
            "Epoch[4/10], Step [310/469], Reconst Loss: 124.0289, KL Div: 5.4055, Entropy: 0.2476, NMI: 0.8312\n",
            "Epoch[4/10], Step [320/469], Reconst Loss: 120.8627, KL Div: 5.2542, Entropy: 0.2206, NMI: 0.9026\n",
            "Epoch[4/10], Step [330/469], Reconst Loss: 122.8020, KL Div: 5.4239, Entropy: 0.3266, NMI: 0.8187\n",
            "Epoch[4/10], Step [340/469], Reconst Loss: 121.4673, KL Div: 5.6690, Entropy: 0.1547, NMI: 0.9252\n",
            "Epoch[4/10], Step [350/469], Reconst Loss: 117.9353, KL Div: 5.4862, Entropy: 0.2113, NMI: 0.8925\n",
            "Epoch[4/10], Step [360/469], Reconst Loss: 118.9788, KL Div: 5.1582, Entropy: 0.2405, NMI: 0.8834\n",
            "Epoch[4/10], Step [370/469], Reconst Loss: 121.1643, KL Div: 5.4651, Entropy: 0.1338, NMI: 0.9357\n",
            "Epoch[4/10], Step [380/469], Reconst Loss: 118.5663, KL Div: 5.4038, Entropy: 0.2036, NMI: 0.9190\n",
            "Epoch[4/10], Step [390/469], Reconst Loss: 121.4554, KL Div: 5.4335, Entropy: 0.2609, NMI: 0.8675\n",
            "Epoch[4/10], Step [400/469], Reconst Loss: 116.4406, KL Div: 5.3391, Entropy: 0.2354, NMI: 0.8813\n",
            "Epoch[4/10], Step [410/469], Reconst Loss: 121.1299, KL Div: 5.6256, Entropy: 0.1773, NMI: 0.9403\n",
            "Epoch[4/10], Step [420/469], Reconst Loss: 128.1556, KL Div: 5.5730, Entropy: 0.4420, NMI: 0.8158\n",
            "Epoch[4/10], Step [430/469], Reconst Loss: 122.8580, KL Div: 5.4535, Entropy: 0.1516, NMI: 0.9546\n",
            "Epoch[4/10], Step [440/469], Reconst Loss: 122.8771, KL Div: 5.5285, Entropy: 0.2571, NMI: 0.8799\n",
            "Epoch[4/10], Step [450/469], Reconst Loss: 118.7488, KL Div: 5.5217, Entropy: 0.1277, NMI: 0.9284\n",
            "Epoch[4/10], Step [460/469], Reconst Loss: 122.8441, KL Div: 5.6017, Entropy: 0.1305, NMI: 0.9405\n",
            "Epoch[5/10], Step [10/469], Reconst Loss: 122.5914, KL Div: 5.2080, Entropy: 0.2199, NMI: 0.8561\n",
            "Epoch[5/10], Step [20/469], Reconst Loss: 118.7938, KL Div: 5.3726, Entropy: 0.2191, NMI: 0.9402\n",
            "Epoch[5/10], Step [30/469], Reconst Loss: 119.5409, KL Div: 5.6406, Entropy: 0.1917, NMI: 0.9053\n",
            "Epoch[5/10], Step [40/469], Reconst Loss: 121.7194, KL Div: 5.5662, Entropy: 0.2433, NMI: 0.8854\n",
            "Epoch[5/10], Step [50/469], Reconst Loss: 123.2928, KL Div: 5.4838, Entropy: 0.2280, NMI: 0.8774\n",
            "Epoch[5/10], Step [60/469], Reconst Loss: 121.6806, KL Div: 5.5263, Entropy: 0.2374, NMI: 0.9075\n",
            "Epoch[5/10], Step [70/469], Reconst Loss: 123.8180, KL Div: 5.5941, Entropy: 0.1407, NMI: 0.9160\n",
            "Epoch[5/10], Step [80/469], Reconst Loss: 116.3325, KL Div: 5.1808, Entropy: 0.1383, NMI: 0.9425\n",
            "Epoch[5/10], Step [90/469], Reconst Loss: 115.5605, KL Div: 5.2819, Entropy: 0.2226, NMI: 0.9032\n",
            "Epoch[5/10], Step [100/469], Reconst Loss: 116.0430, KL Div: 5.1653, Entropy: 0.1967, NMI: 0.9247\n",
            "Epoch[5/10], Step [110/469], Reconst Loss: 127.3711, KL Div: 5.6534, Entropy: 0.2275, NMI: 0.9278\n",
            "Epoch[5/10], Step [120/469], Reconst Loss: 120.0195, KL Div: 5.5170, Entropy: 0.0986, NMI: 0.9651\n",
            "Epoch[5/10], Step [130/469], Reconst Loss: 123.6992, KL Div: 5.7572, Entropy: 0.2588, NMI: 0.8495\n",
            "Epoch[5/10], Step [140/469], Reconst Loss: 124.7507, KL Div: 5.4820, Entropy: 0.2334, NMI: 0.8812\n",
            "Epoch[5/10], Step [150/469], Reconst Loss: 114.7540, KL Div: 5.4536, Entropy: 0.2283, NMI: 0.8662\n",
            "Epoch[5/10], Step [160/469], Reconst Loss: 122.3451, KL Div: 5.7985, Entropy: 0.1518, NMI: 0.8966\n",
            "Epoch[5/10], Step [170/469], Reconst Loss: 116.8802, KL Div: 5.3281, Entropy: 0.1350, NMI: 0.9234\n",
            "Epoch[5/10], Step [180/469], Reconst Loss: 123.4275, KL Div: 5.7088, Entropy: 0.1049, NMI: 0.9511\n",
            "Epoch[5/10], Step [190/469], Reconst Loss: 121.1943, KL Div: 5.2152, Entropy: 0.3440, NMI: 0.8443\n",
            "Epoch[5/10], Step [200/469], Reconst Loss: 115.1737, KL Div: 5.6393, Entropy: 0.1516, NMI: 0.9294\n",
            "Epoch[5/10], Step [210/469], Reconst Loss: 119.5378, KL Div: 5.4036, Entropy: 0.1741, NMI: 0.9172\n",
            "Epoch[5/10], Step [220/469], Reconst Loss: 120.3446, KL Div: 5.3190, Entropy: 0.2825, NMI: 0.9170\n",
            "Epoch[5/10], Step [230/469], Reconst Loss: 117.2154, KL Div: 5.1376, Entropy: 0.1925, NMI: 0.8814\n",
            "Epoch[5/10], Step [240/469], Reconst Loss: 123.4883, KL Div: 5.5000, Entropy: 0.3132, NMI: 0.8654\n",
            "Epoch[5/10], Step [250/469], Reconst Loss: 119.9691, KL Div: 5.4108, Entropy: 0.1443, NMI: 0.9303\n",
            "Epoch[5/10], Step [260/469], Reconst Loss: 115.2514, KL Div: 5.5330, Entropy: 0.1529, NMI: 0.9141\n",
            "Epoch[5/10], Step [270/469], Reconst Loss: 122.5501, KL Div: 5.5868, Entropy: 0.1719, NMI: 0.9089\n",
            "Epoch[5/10], Step [280/469], Reconst Loss: 116.0376, KL Div: 4.9686, Entropy: 0.1711, NMI: 0.8969\n",
            "Epoch[5/10], Step [290/469], Reconst Loss: 113.1992, KL Div: 5.3631, Entropy: 0.1377, NMI: 0.9314\n",
            "Epoch[5/10], Step [300/469], Reconst Loss: 120.2963, KL Div: 5.3123, Entropy: 0.1328, NMI: 0.9254\n",
            "Epoch[5/10], Step [310/469], Reconst Loss: 110.3753, KL Div: 5.2962, Entropy: 0.2023, NMI: 0.8924\n",
            "Epoch[5/10], Step [320/469], Reconst Loss: 114.6569, KL Div: 5.2687, Entropy: 0.1787, NMI: 0.9289\n",
            "Epoch[5/10], Step [330/469], Reconst Loss: 118.5634, KL Div: 5.6678, Entropy: 0.2573, NMI: 0.8476\n",
            "Epoch[5/10], Step [340/469], Reconst Loss: 117.2668, KL Div: 5.2576, Entropy: 0.2500, NMI: 0.8862\n",
            "Epoch[5/10], Step [350/469], Reconst Loss: 117.3853, KL Div: 5.3464, Entropy: 0.1957, NMI: 0.9064\n",
            "Epoch[5/10], Step [360/469], Reconst Loss: 115.5394, KL Div: 5.4303, Entropy: 0.1012, NMI: 0.9574\n",
            "Epoch[5/10], Step [370/469], Reconst Loss: 119.3944, KL Div: 5.5812, Entropy: 0.1808, NMI: 0.9266\n",
            "Epoch[5/10], Step [380/469], Reconst Loss: 123.6420, KL Div: 5.5368, Entropy: 0.1323, NMI: 0.9530\n",
            "Epoch[5/10], Step [390/469], Reconst Loss: 111.5191, KL Div: 5.5147, Entropy: 0.1768, NMI: 0.9478\n",
            "Epoch[5/10], Step [400/469], Reconst Loss: 117.4402, KL Div: 5.3013, Entropy: 0.1017, NMI: 0.9451\n",
            "Epoch[5/10], Step [410/469], Reconst Loss: 124.9612, KL Div: 5.9245, Entropy: 0.3010, NMI: 0.8397\n",
            "Epoch[5/10], Step [420/469], Reconst Loss: 117.3021, KL Div: 5.6495, Entropy: 0.1928, NMI: 0.9154\n",
            "Epoch[5/10], Step [430/469], Reconst Loss: 114.4019, KL Div: 5.3746, Entropy: 0.1708, NMI: 0.9511\n",
            "Epoch[5/10], Step [440/469], Reconst Loss: 124.9039, KL Div: 5.5252, Entropy: 0.1375, NMI: 0.9276\n",
            "Epoch[5/10], Step [450/469], Reconst Loss: 121.0408, KL Div: 5.5086, Entropy: 0.1704, NMI: 0.8970\n",
            "Epoch[5/10], Step [460/469], Reconst Loss: 115.4633, KL Div: 5.5979, Entropy: 0.2057, NMI: 0.8720\n",
            "Epoch[6/10], Step [10/469], Reconst Loss: 116.8316, KL Div: 5.6246, Entropy: 0.1215, NMI: 0.9385\n",
            "Epoch[6/10], Step [20/469], Reconst Loss: 116.9365, KL Div: 5.5026, Entropy: 0.1153, NMI: 0.9289\n",
            "Epoch[6/10], Step [30/469], Reconst Loss: 116.7855, KL Div: 5.6433, Entropy: 0.1943, NMI: 0.9016\n",
            "Epoch[6/10], Step [40/469], Reconst Loss: 123.4338, KL Div: 5.6908, Entropy: 0.1853, NMI: 0.8898\n",
            "Epoch[6/10], Step [50/469], Reconst Loss: 119.8581, KL Div: 5.9093, Entropy: 0.2440, NMI: 0.9053\n",
            "Epoch[6/10], Step [60/469], Reconst Loss: 112.6556, KL Div: 5.3932, Entropy: 0.1055, NMI: 0.9523\n",
            "Epoch[6/10], Step [70/469], Reconst Loss: 118.4603, KL Div: 5.6108, Entropy: 0.2847, NMI: 0.8372\n",
            "Epoch[6/10], Step [80/469], Reconst Loss: 114.3084, KL Div: 5.5236, Entropy: 0.1545, NMI: 0.9443\n",
            "Epoch[6/10], Step [90/469], Reconst Loss: 123.4031, KL Div: 5.9716, Entropy: 0.1835, NMI: 0.8969\n",
            "Epoch[6/10], Step [100/469], Reconst Loss: 119.3093, KL Div: 5.6260, Entropy: 0.2702, NMI: 0.8853\n",
            "Epoch[6/10], Step [110/469], Reconst Loss: 116.8578, KL Div: 5.6165, Entropy: 0.2199, NMI: 0.8731\n",
            "Epoch[6/10], Step [120/469], Reconst Loss: 117.5323, KL Div: 5.6068, Entropy: 0.1593, NMI: 0.8904\n",
            "Epoch[6/10], Step [130/469], Reconst Loss: 117.3558, KL Div: 5.6960, Entropy: 0.1625, NMI: 0.8959\n",
            "Epoch[6/10], Step [140/469], Reconst Loss: 113.4030, KL Div: 5.3799, Entropy: 0.1509, NMI: 0.9327\n",
            "Epoch[6/10], Step [150/469], Reconst Loss: 117.9376, KL Div: 5.4591, Entropy: 0.1852, NMI: 0.9019\n",
            "Epoch[6/10], Step [160/469], Reconst Loss: 117.1240, KL Div: 5.8904, Entropy: 0.2034, NMI: 0.9038\n",
            "Epoch[6/10], Step [170/469], Reconst Loss: 116.7588, KL Div: 5.5641, Entropy: 0.2336, NMI: 0.8906\n",
            "Epoch[6/10], Step [180/469], Reconst Loss: 116.9511, KL Div: 5.7430, Entropy: 0.1781, NMI: 0.9181\n",
            "Epoch[6/10], Step [190/469], Reconst Loss: 114.7608, KL Div: 5.7068, Entropy: 0.2248, NMI: 0.9338\n",
            "Epoch[6/10], Step [200/469], Reconst Loss: 117.7158, KL Div: 5.0564, Entropy: 0.1950, NMI: 0.8883\n",
            "Epoch[6/10], Step [210/469], Reconst Loss: 115.6370, KL Div: 5.5900, Entropy: 0.1538, NMI: 0.9411\n",
            "Epoch[6/10], Step [220/469], Reconst Loss: 119.6139, KL Div: 5.5576, Entropy: 0.1582, NMI: 0.8929\n",
            "Epoch[6/10], Step [230/469], Reconst Loss: 122.2590, KL Div: 5.8328, Entropy: 0.2362, NMI: 0.9165\n",
            "Epoch[6/10], Step [240/469], Reconst Loss: 115.3344, KL Div: 5.1691, Entropy: 0.2957, NMI: 0.8817\n",
            "Epoch[6/10], Step [250/469], Reconst Loss: 114.7324, KL Div: 5.4999, Entropy: 0.1319, NMI: 0.9420\n",
            "Epoch[6/10], Step [260/469], Reconst Loss: 116.1832, KL Div: 5.5322, Entropy: 0.1397, NMI: 0.9153\n",
            "Epoch[6/10], Step [270/469], Reconst Loss: 122.8838, KL Div: 6.0256, Entropy: 0.2856, NMI: 0.8829\n",
            "Epoch[6/10], Step [280/469], Reconst Loss: 113.7784, KL Div: 5.6555, Entropy: 0.1643, NMI: 0.9276\n",
            "Epoch[6/10], Step [290/469], Reconst Loss: 118.7663, KL Div: 5.5565, Entropy: 0.2185, NMI: 0.8849\n",
            "Epoch[6/10], Step [300/469], Reconst Loss: 111.5674, KL Div: 5.4775, Entropy: 0.2812, NMI: 0.8540\n",
            "Epoch[6/10], Step [310/469], Reconst Loss: 117.4863, KL Div: 5.7305, Entropy: 0.2148, NMI: 0.8830\n",
            "Epoch[6/10], Step [320/469], Reconst Loss: 123.6688, KL Div: 5.5123, Entropy: 0.1840, NMI: 0.8749\n",
            "Epoch[6/10], Step [330/469], Reconst Loss: 113.3659, KL Div: 5.4717, Entropy: 0.2673, NMI: 0.8905\n",
            "Epoch[6/10], Step [340/469], Reconst Loss: 120.1196, KL Div: 5.8180, Entropy: 0.3040, NMI: 0.8733\n",
            "Epoch[6/10], Step [350/469], Reconst Loss: 114.0477, KL Div: 5.8405, Entropy: 0.1040, NMI: 0.9654\n",
            "Epoch[6/10], Step [360/469], Reconst Loss: 123.8807, KL Div: 5.7085, Entropy: 0.1743, NMI: 0.9034\n",
            "Epoch[6/10], Step [370/469], Reconst Loss: 119.1788, KL Div: 5.7038, Entropy: 0.1574, NMI: 0.9386\n",
            "Epoch[6/10], Step [380/469], Reconst Loss: 108.5820, KL Div: 5.6117, Entropy: 0.0891, NMI: 0.9512\n",
            "Epoch[6/10], Step [390/469], Reconst Loss: 115.9214, KL Div: 5.6547, Entropy: 0.1627, NMI: 0.9078\n",
            "Epoch[6/10], Step [400/469], Reconst Loss: 123.1379, KL Div: 5.6570, Entropy: 0.2303, NMI: 0.9280\n",
            "Epoch[6/10], Step [410/469], Reconst Loss: 115.3703, KL Div: 5.5992, Entropy: 0.1748, NMI: 0.9130\n",
            "Epoch[6/10], Step [420/469], Reconst Loss: 120.1863, KL Div: 5.6926, Entropy: 0.1902, NMI: 0.9379\n",
            "Epoch[6/10], Step [430/469], Reconst Loss: 116.2298, KL Div: 5.6597, Entropy: 0.1380, NMI: 0.9376\n",
            "Epoch[6/10], Step [440/469], Reconst Loss: 118.3198, KL Div: 5.9832, Entropy: 0.2194, NMI: 0.9275\n",
            "Epoch[6/10], Step [450/469], Reconst Loss: 116.4718, KL Div: 5.6246, Entropy: 0.2259, NMI: 0.9206\n",
            "Epoch[6/10], Step [460/469], Reconst Loss: 116.6269, KL Div: 5.2286, Entropy: 0.0634, NMI: 0.9756\n",
            "Epoch[7/10], Step [10/469], Reconst Loss: 118.8256, KL Div: 5.9489, Entropy: 0.2285, NMI: 0.8961\n",
            "Epoch[7/10], Step [20/469], Reconst Loss: 113.6009, KL Div: 5.4718, Entropy: 0.1901, NMI: 0.9036\n",
            "Epoch[7/10], Step [30/469], Reconst Loss: 117.4127, KL Div: 5.6233, Entropy: 0.1543, NMI: 0.8770\n",
            "Epoch[7/10], Step [40/469], Reconst Loss: 118.3641, KL Div: 5.4756, Entropy: 0.2154, NMI: 0.9079\n",
            "Epoch[7/10], Step [50/469], Reconst Loss: 120.7496, KL Div: 5.7699, Entropy: 0.2113, NMI: 0.9031\n",
            "Epoch[7/10], Step [60/469], Reconst Loss: 116.4103, KL Div: 5.7831, Entropy: 0.1921, NMI: 0.9089\n",
            "Epoch[7/10], Step [70/469], Reconst Loss: 110.6174, KL Div: 5.5198, Entropy: 0.1555, NMI: 0.8996\n",
            "Epoch[7/10], Step [80/469], Reconst Loss: 122.7294, KL Div: 5.5641, Entropy: 0.1892, NMI: 0.8914\n",
            "Epoch[7/10], Step [90/469], Reconst Loss: 112.0121, KL Div: 5.5560, Entropy: 0.3421, NMI: 0.8810\n",
            "Epoch[7/10], Step [100/469], Reconst Loss: 116.2191, KL Div: 5.7352, Entropy: 0.1777, NMI: 0.9046\n",
            "Epoch[7/10], Step [110/469], Reconst Loss: 116.0088, KL Div: 5.7278, Entropy: 0.2325, NMI: 0.8935\n",
            "Epoch[7/10], Step [120/469], Reconst Loss: 112.6954, KL Div: 5.5919, Entropy: 0.1355, NMI: 0.9255\n",
            "Epoch[7/10], Step [130/469], Reconst Loss: 115.4559, KL Div: 5.6189, Entropy: 0.2134, NMI: 0.8683\n",
            "Epoch[7/10], Step [140/469], Reconst Loss: 116.4549, KL Div: 5.9255, Entropy: 0.1061, NMI: 0.9549\n",
            "Epoch[7/10], Step [150/469], Reconst Loss: 111.8367, KL Div: 5.0502, Entropy: 0.0891, NMI: 0.9501\n",
            "Epoch[7/10], Step [160/469], Reconst Loss: 113.3996, KL Div: 5.7398, Entropy: 0.1304, NMI: 0.9252\n",
            "Epoch[7/10], Step [170/469], Reconst Loss: 118.6251, KL Div: 5.9125, Entropy: 0.2782, NMI: 0.9091\n",
            "Epoch[7/10], Step [180/469], Reconst Loss: 113.2627, KL Div: 5.4589, Entropy: 0.1751, NMI: 0.8968\n",
            "Epoch[7/10], Step [190/469], Reconst Loss: 118.7269, KL Div: 5.6467, Entropy: 0.2496, NMI: 0.9208\n",
            "Epoch[7/10], Step [200/469], Reconst Loss: 119.1767, KL Div: 5.6201, Entropy: 0.2585, NMI: 0.8494\n",
            "Epoch[7/10], Step [210/469], Reconst Loss: 116.2388, KL Div: 6.1982, Entropy: 0.1634, NMI: 0.9361\n",
            "Epoch[7/10], Step [220/469], Reconst Loss: 115.4668, KL Div: 5.9625, Entropy: 0.1625, NMI: 0.9070\n",
            "Epoch[7/10], Step [230/469], Reconst Loss: 116.3158, KL Div: 5.7293, Entropy: 0.1127, NMI: 0.9396\n",
            "Epoch[7/10], Step [240/469], Reconst Loss: 114.8670, KL Div: 6.0105, Entropy: 0.1545, NMI: 0.8924\n",
            "Epoch[7/10], Step [250/469], Reconst Loss: 118.0184, KL Div: 5.5476, Entropy: 0.2999, NMI: 0.8077\n",
            "Epoch[7/10], Step [260/469], Reconst Loss: 119.7710, KL Div: 5.9251, Entropy: 0.1781, NMI: 0.9020\n",
            "Epoch[7/10], Step [270/469], Reconst Loss: 116.0466, KL Div: 5.8938, Entropy: 0.1540, NMI: 0.9327\n",
            "Epoch[7/10], Step [280/469], Reconst Loss: 114.7239, KL Div: 5.6530, Entropy: 0.2107, NMI: 0.8921\n",
            "Epoch[7/10], Step [290/469], Reconst Loss: 112.9608, KL Div: 5.7204, Entropy: 0.1327, NMI: 0.9204\n",
            "Epoch[7/10], Step [300/469], Reconst Loss: 115.0566, KL Div: 5.7310, Entropy: 0.1386, NMI: 0.9026\n",
            "Epoch[7/10], Step [310/469], Reconst Loss: 113.8487, KL Div: 5.5566, Entropy: 0.2615, NMI: 0.9237\n",
            "Epoch[7/10], Step [320/469], Reconst Loss: 115.0383, KL Div: 5.8819, Entropy: 0.1999, NMI: 0.9090\n",
            "Epoch[7/10], Step [330/469], Reconst Loss: 114.1065, KL Div: 5.7715, Entropy: 0.1480, NMI: 0.9518\n",
            "Epoch[7/10], Step [340/469], Reconst Loss: 116.7829, KL Div: 5.6118, Entropy: 0.1421, NMI: 0.9268\n",
            "Epoch[7/10], Step [350/469], Reconst Loss: 116.7793, KL Div: 5.8773, Entropy: 0.1311, NMI: 0.9265\n",
            "Epoch[7/10], Step [360/469], Reconst Loss: 123.3546, KL Div: 5.7621, Entropy: 0.1625, NMI: 0.9235\n",
            "Epoch[7/10], Step [370/469], Reconst Loss: 112.7102, KL Div: 5.5122, Entropy: 0.1585, NMI: 0.9407\n",
            "Epoch[7/10], Step [380/469], Reconst Loss: 116.0426, KL Div: 5.8393, Entropy: 0.2496, NMI: 0.8773\n",
            "Epoch[7/10], Step [390/469], Reconst Loss: 115.8788, KL Div: 5.4483, Entropy: 0.2158, NMI: 0.9099\n",
            "Epoch[7/10], Step [400/469], Reconst Loss: 113.2743, KL Div: 5.9644, Entropy: 0.3377, NMI: 0.8924\n",
            "Epoch[7/10], Step [410/469], Reconst Loss: 118.7010, KL Div: 5.7696, Entropy: 0.1675, NMI: 0.9035\n",
            "Epoch[7/10], Step [420/469], Reconst Loss: 115.4379, KL Div: 5.8589, Entropy: 0.3703, NMI: 0.8408\n",
            "Epoch[7/10], Step [430/469], Reconst Loss: 114.7319, KL Div: 5.6777, Entropy: 0.1505, NMI: 0.9512\n",
            "Epoch[7/10], Step [440/469], Reconst Loss: 116.4168, KL Div: 5.6669, Entropy: 0.1228, NMI: 0.9299\n",
            "Epoch[7/10], Step [450/469], Reconst Loss: 116.5725, KL Div: 5.7410, Entropy: 0.1635, NMI: 0.9273\n",
            "Epoch[7/10], Step [460/469], Reconst Loss: 120.8812, KL Div: 5.8450, Entropy: 0.2215, NMI: 0.8961\n",
            "Epoch[8/10], Step [10/469], Reconst Loss: 118.9218, KL Div: 6.0429, Entropy: 0.1776, NMI: 0.8795\n",
            "Epoch[8/10], Step [20/469], Reconst Loss: 113.3338, KL Div: 5.9617, Entropy: 0.1337, NMI: 0.9411\n",
            "Epoch[8/10], Step [30/469], Reconst Loss: 109.1023, KL Div: 5.4693, Entropy: 0.0926, NMI: 0.9632\n",
            "Epoch[8/10], Step [40/469], Reconst Loss: 119.7142, KL Div: 5.9150, Entropy: 0.1739, NMI: 0.8941\n",
            "Epoch[8/10], Step [50/469], Reconst Loss: 115.3978, KL Div: 5.6787, Entropy: 0.1261, NMI: 0.9640\n",
            "Epoch[8/10], Step [60/469], Reconst Loss: 107.9743, KL Div: 5.8390, Entropy: 0.1202, NMI: 0.9528\n",
            "Epoch[8/10], Step [70/469], Reconst Loss: 114.5149, KL Div: 5.3573, Entropy: 0.1836, NMI: 0.9159\n",
            "Epoch[8/10], Step [80/469], Reconst Loss: 113.2584, KL Div: 5.9614, Entropy: 0.1597, NMI: 0.9045\n",
            "Epoch[8/10], Step [90/469], Reconst Loss: 121.3189, KL Div: 5.5981, Entropy: 0.1363, NMI: 0.9402\n",
            "Epoch[8/10], Step [100/469], Reconst Loss: 113.8569, KL Div: 5.7397, Entropy: 0.1194, NMI: 0.9514\n",
            "Epoch[8/10], Step [110/469], Reconst Loss: 110.3614, KL Div: 5.7527, Entropy: 0.1804, NMI: 0.9086\n",
            "Epoch[8/10], Step [120/469], Reconst Loss: 109.9663, KL Div: 5.6874, Entropy: 0.1120, NMI: 0.9501\n",
            "Epoch[8/10], Step [130/469], Reconst Loss: 109.4534, KL Div: 5.7183, Entropy: 0.2523, NMI: 0.8689\n",
            "Epoch[8/10], Step [140/469], Reconst Loss: 115.5055, KL Div: 5.6895, Entropy: 0.1520, NMI: 0.9253\n",
            "Epoch[8/10], Step [150/469], Reconst Loss: 112.5082, KL Div: 5.6851, Entropy: 0.1334, NMI: 0.9311\n",
            "Epoch[8/10], Step [160/469], Reconst Loss: 122.2470, KL Div: 6.0968, Entropy: 0.1804, NMI: 0.9223\n",
            "Epoch[8/10], Step [170/469], Reconst Loss: 113.8638, KL Div: 5.6984, Entropy: 0.1502, NMI: 0.9033\n",
            "Epoch[8/10], Step [180/469], Reconst Loss: 114.1971, KL Div: 5.7617, Entropy: 0.1023, NMI: 0.9639\n",
            "Epoch[8/10], Step [190/469], Reconst Loss: 118.4780, KL Div: 5.6101, Entropy: 0.2092, NMI: 0.9177\n",
            "Epoch[8/10], Step [200/469], Reconst Loss: 116.3452, KL Div: 5.9889, Entropy: 0.2096, NMI: 0.8847\n",
            "Epoch[8/10], Step [210/469], Reconst Loss: 111.7274, KL Div: 5.9611, Entropy: 0.2836, NMI: 0.8845\n",
            "Epoch[8/10], Step [220/469], Reconst Loss: 120.3922, KL Div: 5.9661, Entropy: 0.1308, NMI: 0.9383\n",
            "Epoch[8/10], Step [230/469], Reconst Loss: 108.4715, KL Div: 5.8669, Entropy: 0.1106, NMI: 0.9335\n",
            "Epoch[8/10], Step [240/469], Reconst Loss: 111.5537, KL Div: 5.7685, Entropy: 0.1476, NMI: 0.9164\n",
            "Epoch[8/10], Step [250/469], Reconst Loss: 122.5878, KL Div: 6.3656, Entropy: 0.1886, NMI: 0.8812\n",
            "Epoch[8/10], Step [260/469], Reconst Loss: 116.4390, KL Div: 5.8693, Entropy: 0.1841, NMI: 0.9141\n",
            "Epoch[8/10], Step [270/469], Reconst Loss: 110.9111, KL Div: 6.0335, Entropy: 0.1390, NMI: 0.9116\n",
            "Epoch[8/10], Step [280/469], Reconst Loss: 117.9956, KL Div: 5.7105, Entropy: 0.1791, NMI: 0.9005\n",
            "Epoch[8/10], Step [290/469], Reconst Loss: 114.1661, KL Div: 5.9177, Entropy: 0.0964, NMI: 0.9558\n",
            "Epoch[8/10], Step [300/469], Reconst Loss: 113.3503, KL Div: 5.8210, Entropy: 0.1829, NMI: 0.9158\n",
            "Epoch[8/10], Step [310/469], Reconst Loss: 116.0812, KL Div: 5.9901, Entropy: 0.1979, NMI: 0.9082\n",
            "Epoch[8/10], Step [320/469], Reconst Loss: 108.9483, KL Div: 5.6960, Entropy: 0.1788, NMI: 0.8894\n",
            "Epoch[8/10], Step [330/469], Reconst Loss: 115.1284, KL Div: 5.5462, Entropy: 0.2361, NMI: 0.9155\n",
            "Epoch[8/10], Step [340/469], Reconst Loss: 110.6830, KL Div: 5.7556, Entropy: 0.1114, NMI: 0.9629\n",
            "Epoch[8/10], Step [350/469], Reconst Loss: 110.9319, KL Div: 5.6909, Entropy: 0.2035, NMI: 0.8704\n",
            "Epoch[8/10], Step [360/469], Reconst Loss: 112.0845, KL Div: 5.7052, Entropy: 0.1170, NMI: 0.9404\n",
            "Epoch[8/10], Step [370/469], Reconst Loss: 109.6500, KL Div: 5.9233, Entropy: 0.1741, NMI: 0.9180\n",
            "Epoch[8/10], Step [380/469], Reconst Loss: 114.4090, KL Div: 5.3557, Entropy: 0.2044, NMI: 0.8844\n",
            "Epoch[8/10], Step [390/469], Reconst Loss: 114.3711, KL Div: 5.6941, Entropy: 0.2233, NMI: 0.8830\n",
            "Epoch[8/10], Step [400/469], Reconst Loss: 112.9497, KL Div: 5.7972, Entropy: 0.1764, NMI: 0.9310\n",
            "Epoch[8/10], Step [410/469], Reconst Loss: 118.2473, KL Div: 5.9072, Entropy: 0.1721, NMI: 0.9278\n",
            "Epoch[8/10], Step [420/469], Reconst Loss: 113.2878, KL Div: 6.1001, Entropy: 0.1532, NMI: 0.9159\n",
            "Epoch[8/10], Step [430/469], Reconst Loss: 117.0116, KL Div: 5.8838, Entropy: 0.1766, NMI: 0.9021\n",
            "Epoch[8/10], Step [440/469], Reconst Loss: 112.9457, KL Div: 5.7077, Entropy: 0.1908, NMI: 0.9506\n",
            "Epoch[8/10], Step [450/469], Reconst Loss: 114.8872, KL Div: 5.7260, Entropy: 0.1291, NMI: 0.9453\n",
            "Epoch[8/10], Step [460/469], Reconst Loss: 113.2985, KL Div: 6.0151, Entropy: 0.2307, NMI: 0.8684\n",
            "Epoch[9/10], Step [10/469], Reconst Loss: 113.0028, KL Div: 5.6323, Entropy: 0.1687, NMI: 0.9259\n",
            "Epoch[9/10], Step [20/469], Reconst Loss: 115.3344, KL Div: 5.8951, Entropy: 0.2012, NMI: 0.8686\n",
            "Epoch[9/10], Step [30/469], Reconst Loss: 112.9820, KL Div: 6.0429, Entropy: 0.0766, NMI: 0.9614\n",
            "Epoch[9/10], Step [40/469], Reconst Loss: 115.9567, KL Div: 5.9210, Entropy: 0.1099, NMI: 0.9528\n",
            "Epoch[9/10], Step [50/469], Reconst Loss: 109.5227, KL Div: 5.4202, Entropy: 0.1990, NMI: 0.9387\n",
            "Epoch[9/10], Step [60/469], Reconst Loss: 118.3548, KL Div: 5.7451, Entropy: 0.2729, NMI: 0.8679\n",
            "Epoch[9/10], Step [70/469], Reconst Loss: 107.7499, KL Div: 5.9142, Entropy: 0.1229, NMI: 0.9382\n",
            "Epoch[9/10], Step [80/469], Reconst Loss: 116.8844, KL Div: 5.6349, Entropy: 0.1595, NMI: 0.8896\n",
            "Epoch[9/10], Step [90/469], Reconst Loss: 116.2337, KL Div: 5.7804, Entropy: 0.1751, NMI: 0.9046\n",
            "Epoch[9/10], Step [100/469], Reconst Loss: 113.7363, KL Div: 5.6532, Entropy: 0.1942, NMI: 0.9035\n",
            "Epoch[9/10], Step [110/469], Reconst Loss: 110.7125, KL Div: 5.7970, Entropy: 0.1812, NMI: 0.8860\n",
            "Epoch[9/10], Step [120/469], Reconst Loss: 115.1267, KL Div: 5.9405, Entropy: 0.1155, NMI: 0.9390\n",
            "Epoch[9/10], Step [130/469], Reconst Loss: 116.8736, KL Div: 5.9735, Entropy: 0.1394, NMI: 0.9035\n",
            "Epoch[9/10], Step [140/469], Reconst Loss: 117.4944, KL Div: 5.8639, Entropy: 0.1543, NMI: 0.9285\n",
            "Epoch[9/10], Step [150/469], Reconst Loss: 115.6492, KL Div: 6.1420, Entropy: 0.1718, NMI: 0.8743\n",
            "Epoch[9/10], Step [160/469], Reconst Loss: 114.4584, KL Div: 6.2313, Entropy: 0.1421, NMI: 0.9206\n",
            "Epoch[9/10], Step [170/469], Reconst Loss: 116.1253, KL Div: 5.7676, Entropy: 0.1735, NMI: 0.9304\n",
            "Epoch[9/10], Step [180/469], Reconst Loss: 110.9799, KL Div: 5.9447, Entropy: 0.1142, NMI: 0.9407\n",
            "Epoch[9/10], Step [190/469], Reconst Loss: 109.5279, KL Div: 5.5253, Entropy: 0.1309, NMI: 0.9325\n",
            "Epoch[9/10], Step [200/469], Reconst Loss: 112.0701, KL Div: 5.9363, Entropy: 0.0871, NMI: 0.9506\n",
            "Epoch[9/10], Step [210/469], Reconst Loss: 111.1904, KL Div: 5.6097, Entropy: 0.1554, NMI: 0.9258\n",
            "Epoch[9/10], Step [220/469], Reconst Loss: 118.8215, KL Div: 5.9118, Entropy: 0.2320, NMI: 0.8819\n",
            "Epoch[9/10], Step [230/469], Reconst Loss: 118.7138, KL Div: 5.8866, Entropy: 0.2090, NMI: 0.8968\n",
            "Epoch[9/10], Step [240/469], Reconst Loss: 115.1601, KL Div: 5.8056, Entropy: 0.1119, NMI: 0.9533\n",
            "Epoch[9/10], Step [250/469], Reconst Loss: 113.3712, KL Div: 5.7025, Entropy: 0.2752, NMI: 0.9176\n",
            "Epoch[9/10], Step [260/469], Reconst Loss: 115.5416, KL Div: 6.1579, Entropy: 0.1748, NMI: 0.9197\n",
            "Epoch[9/10], Step [270/469], Reconst Loss: 112.3905, KL Div: 5.9590, Entropy: 0.0858, NMI: 0.9743\n",
            "Epoch[9/10], Step [280/469], Reconst Loss: 115.2742, KL Div: 5.9823, Entropy: 0.0539, NMI: 0.9758\n",
            "Epoch[9/10], Step [290/469], Reconst Loss: 112.9136, KL Div: 5.5813, Entropy: 0.2749, NMI: 0.8794\n",
            "Epoch[9/10], Step [300/469], Reconst Loss: 110.7345, KL Div: 5.7209, Entropy: 0.1567, NMI: 0.8783\n",
            "Epoch[9/10], Step [310/469], Reconst Loss: 111.7002, KL Div: 5.9578, Entropy: 0.1532, NMI: 0.9328\n",
            "Epoch[9/10], Step [320/469], Reconst Loss: 115.4112, KL Div: 5.7561, Entropy: 0.1770, NMI: 0.9111\n",
            "Epoch[9/10], Step [330/469], Reconst Loss: 115.3349, KL Div: 5.8794, Entropy: 0.1027, NMI: 0.9238\n",
            "Epoch[9/10], Step [340/469], Reconst Loss: 110.0070, KL Div: 5.9030, Entropy: 0.1448, NMI: 0.9239\n",
            "Epoch[9/10], Step [350/469], Reconst Loss: 113.4196, KL Div: 5.8997, Entropy: 0.1433, NMI: 0.9312\n",
            "Epoch[9/10], Step [360/469], Reconst Loss: 115.0414, KL Div: 5.6993, Entropy: 0.2122, NMI: 0.8945\n",
            "Epoch[9/10], Step [370/469], Reconst Loss: 109.4023, KL Div: 5.7310, Entropy: 0.1125, NMI: 0.9505\n",
            "Epoch[9/10], Step [380/469], Reconst Loss: 109.4253, KL Div: 5.7921, Entropy: 0.0576, NMI: 0.9882\n",
            "Epoch[9/10], Step [390/469], Reconst Loss: 113.3463, KL Div: 6.0057, Entropy: 0.2142, NMI: 0.8822\n",
            "Epoch[9/10], Step [400/469], Reconst Loss: 121.9794, KL Div: 5.9956, Entropy: 0.1402, NMI: 0.9263\n",
            "Epoch[9/10], Step [410/469], Reconst Loss: 113.9866, KL Div: 5.9893, Entropy: 0.2518, NMI: 0.9123\n",
            "Epoch[9/10], Step [420/469], Reconst Loss: 116.1977, KL Div: 5.6870, Entropy: 0.1725, NMI: 0.9405\n",
            "Epoch[9/10], Step [430/469], Reconst Loss: 112.6462, KL Div: 5.9722, Entropy: 0.1568, NMI: 0.9008\n",
            "Epoch[9/10], Step [440/469], Reconst Loss: 114.6001, KL Div: 5.7562, Entropy: 0.2641, NMI: 0.8714\n",
            "Epoch[9/10], Step [450/469], Reconst Loss: 108.6387, KL Div: 5.8873, Entropy: 0.1035, NMI: 0.9405\n",
            "Epoch[9/10], Step [460/469], Reconst Loss: 113.6086, KL Div: 6.0337, Entropy: 0.2268, NMI: 0.8856\n",
            "Epoch[10/10], Step [10/469], Reconst Loss: 117.1383, KL Div: 6.0047, Entropy: 0.2588, NMI: 0.8828\n",
            "Epoch[10/10], Step [20/469], Reconst Loss: 109.3538, KL Div: 5.6289, Entropy: 0.1691, NMI: 0.9270\n",
            "Epoch[10/10], Step [30/469], Reconst Loss: 120.2794, KL Div: 6.1927, Entropy: 0.1549, NMI: 0.9289\n",
            "Epoch[10/10], Step [40/469], Reconst Loss: 110.2249, KL Div: 5.6043, Entropy: 0.1416, NMI: 0.9083\n",
            "Epoch[10/10], Step [50/469], Reconst Loss: 116.4624, KL Div: 5.9777, Entropy: 0.1568, NMI: 0.9121\n",
            "Epoch[10/10], Step [60/469], Reconst Loss: 113.5862, KL Div: 6.0950, Entropy: 0.1955, NMI: 0.8911\n",
            "Epoch[10/10], Step [70/469], Reconst Loss: 113.0462, KL Div: 6.0212, Entropy: 0.1531, NMI: 0.9279\n",
            "Epoch[10/10], Step [80/469], Reconst Loss: 109.1043, KL Div: 5.7212, Entropy: 0.1113, NMI: 0.9408\n",
            "Epoch[10/10], Step [90/469], Reconst Loss: 113.0639, KL Div: 5.6183, Entropy: 0.2694, NMI: 0.8633\n",
            "Epoch[10/10], Step [100/469], Reconst Loss: 109.5130, KL Div: 6.0921, Entropy: 0.1061, NMI: 0.9510\n",
            "Epoch[10/10], Step [110/469], Reconst Loss: 113.1763, KL Div: 5.9727, Entropy: 0.2037, NMI: 0.9093\n",
            "Epoch[10/10], Step [120/469], Reconst Loss: 115.1543, KL Div: 6.1199, Entropy: 0.2066, NMI: 0.9041\n",
            "Epoch[10/10], Step [130/469], Reconst Loss: 116.3093, KL Div: 5.9830, Entropy: 0.1534, NMI: 0.9411\n",
            "Epoch[10/10], Step [140/469], Reconst Loss: 113.4195, KL Div: 6.0330, Entropy: 0.1869, NMI: 0.9501\n",
            "Epoch[10/10], Step [150/469], Reconst Loss: 113.2500, KL Div: 5.9288, Entropy: 0.1013, NMI: 0.9391\n",
            "Epoch[10/10], Step [160/469], Reconst Loss: 115.2360, KL Div: 5.5889, Entropy: 0.1396, NMI: 0.9388\n",
            "Epoch[10/10], Step [170/469], Reconst Loss: 114.3614, KL Div: 5.9339, Entropy: 0.1687, NMI: 0.9046\n",
            "Epoch[10/10], Step [180/469], Reconst Loss: 109.2108, KL Div: 5.5017, Entropy: 0.0837, NMI: 0.9638\n",
            "Epoch[10/10], Step [190/469], Reconst Loss: 111.3004, KL Div: 6.0170, Entropy: 0.0712, NMI: 0.9630\n",
            "Epoch[10/10], Step [200/469], Reconst Loss: 114.4668, KL Div: 5.8194, Entropy: 0.1215, NMI: 0.9389\n",
            "Epoch[10/10], Step [210/469], Reconst Loss: 118.4936, KL Div: 5.9308, Entropy: 0.1189, NMI: 0.9143\n",
            "Epoch[10/10], Step [220/469], Reconst Loss: 113.0592, KL Div: 5.7773, Entropy: 0.2757, NMI: 0.9034\n",
            "Epoch[10/10], Step [230/469], Reconst Loss: 110.4739, KL Div: 5.9304, Entropy: 0.1279, NMI: 0.9210\n",
            "Epoch[10/10], Step [240/469], Reconst Loss: 117.3338, KL Div: 6.0651, Entropy: 0.1484, NMI: 0.9067\n",
            "Epoch[10/10], Step [250/469], Reconst Loss: 114.8183, KL Div: 5.9197, Entropy: 0.2309, NMI: 0.8538\n",
            "Epoch[10/10], Step [260/469], Reconst Loss: 117.4308, KL Div: 6.0793, Entropy: 0.1070, NMI: 0.9500\n",
            "Epoch[10/10], Step [270/469], Reconst Loss: 111.3765, KL Div: 5.6548, Entropy: 0.1427, NMI: 0.9146\n",
            "Epoch[10/10], Step [280/469], Reconst Loss: 113.8388, KL Div: 6.2225, Entropy: 0.1563, NMI: 0.8746\n",
            "Epoch[10/10], Step [290/469], Reconst Loss: 114.8653, KL Div: 6.0306, Entropy: 0.1230, NMI: 0.9187\n",
            "Epoch[10/10], Step [300/469], Reconst Loss: 110.6394, KL Div: 5.6606, Entropy: 0.1212, NMI: 0.9284\n",
            "Epoch[10/10], Step [310/469], Reconst Loss: 112.5675, KL Div: 5.5957, Entropy: 0.1069, NMI: 0.9293\n",
            "Epoch[10/10], Step [320/469], Reconst Loss: 113.6375, KL Div: 5.8572, Entropy: 0.1642, NMI: 0.9080\n",
            "Epoch[10/10], Step [330/469], Reconst Loss: 115.4511, KL Div: 5.7731, Entropy: 0.1246, NMI: 0.9564\n",
            "Epoch[10/10], Step [340/469], Reconst Loss: 109.4032, KL Div: 5.8963, Entropy: 0.1004, NMI: 0.9630\n",
            "Epoch[10/10], Step [350/469], Reconst Loss: 114.3347, KL Div: 6.2445, Entropy: 0.2253, NMI: 0.8807\n",
            "Epoch[10/10], Step [360/469], Reconst Loss: 107.1814, KL Div: 5.7948, Entropy: 0.0951, NMI: 0.9767\n",
            "Epoch[10/10], Step [370/469], Reconst Loss: 112.4222, KL Div: 5.9813, Entropy: 0.0594, NMI: 0.9755\n",
            "Epoch[10/10], Step [380/469], Reconst Loss: 111.8685, KL Div: 5.6949, Entropy: 0.0836, NMI: 0.9441\n",
            "Epoch[10/10], Step [390/469], Reconst Loss: 114.0746, KL Div: 6.0924, Entropy: 0.0619, NMI: 0.9752\n",
            "Epoch[10/10], Step [400/469], Reconst Loss: 116.6094, KL Div: 6.0752, Entropy: 0.1442, NMI: 0.9343\n",
            "Epoch[10/10], Step [410/469], Reconst Loss: 114.6667, KL Div: 5.8933, Entropy: 0.1214, NMI: 0.9331\n",
            "Epoch[10/10], Step [420/469], Reconst Loss: 117.5587, KL Div: 6.2999, Entropy: 0.1662, NMI: 0.8899\n",
            "Epoch[10/10], Step [430/469], Reconst Loss: 110.7421, KL Div: 5.5823, Entropy: 0.1169, NMI: 0.9049\n",
            "Epoch[10/10], Step [440/469], Reconst Loss: 112.3365, KL Div: 6.0866, Entropy: 0.1326, NMI: 0.9305\n",
            "Epoch[10/10], Step [450/469], Reconst Loss: 113.5904, KL Div: 5.7064, Entropy: 0.1649, NMI: 0.9272\n",
            "Epoch[10/10], Step [460/469], Reconst Loss: 108.4867, KL Div: 5.8999, Entropy: 0.1198, NMI: 0.9648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a1wPoOXq8hy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = np.zeros((8,n_classes))\n",
        "matrix[:,0] = 1\n",
        "final = matrix[:]\n",
        "for i in range(1,n_classes):\n",
        "    final = np.vstack((final,np.roll(matrix,i)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3MsdEzo27WBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "62d9b3ab-70fe-43c6-d98e-ef6b2e6cad65"
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        z = torch.randn(8*n_classes, z_dim).to(device)\n",
        "        y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
        "        out = model_G.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
        "out_grid = torchvision.utils.make_grid(out).cpu()\n",
        "show(out_grid)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAAD8CAYAAAAPIYpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7720d4ae10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ImnXSXUz7nQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "90561df2-7ea6-4b7f-c644-5ba0156c4200"
      },
      "cell_type": "code",
      "source": [
        "x,_ = next(iter(data_loader))\n",
        "x = x[:24,:,:,:].to(device)\n",
        "out, _, _, log_p = model_G(x.view(-1, image_size)) \n",
        "x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
        "out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
        "show(out_grid)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAABlCAYAAACC0GLGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7720d4aa58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Iyd-yXm5JUEi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I tried different beta, C_z, C_c while keeping epochs to 10. I found that beta = 4., C_z = 5, C_c = 10 is quite alright. We still see that \"ghost\" effect. These numbers are heuristical because I found them only by looking generation and reconstruction, but for better results we could take into account ours losses and try with several beta, C_z and C_c ( gradually increase from one value to another one ) to get the lowest loss !\n",
        "\n",
        "The structure of our network is very simple. Indeed, we only use FC layer but we could do Conv and Deconv layers ( because we are working with images ) and these layers take more spatial information which is very good for images.\n",
        "\n",
        "And In addition, if generation is what we really want, we could use a GAN structure which is very effective these days for generating new images that looks like real !\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Y6wvUaK48Mti",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
